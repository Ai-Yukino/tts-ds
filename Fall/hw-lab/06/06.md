# `06.md`

[ðŸ”— Towards data science episode 46](https://titus.techtalentsouth.com/mod/page/view.php?id=53735)

---

## Notes

Data cleaning is somehow separate from ETL (extract, transform, load).

Automatatically filling in missing data (matrix completion, netflix problem)

---

## Write-up

One take-away: data science pipelines should closely incorporate data cleaning into their designs.
Apparently it's not easy to separate data cleaning as a separate "transform" box that you can plug in and ignore.

The speaker also mentions probabilistically predicing missing data. He doesn't provide any algorithmic or mathematical details, though.

The speaker also mentions that the cleaning goal may not be clear as well. So the entire data process may feed into the data cleaning process, and thus the data cleaning process should adapt overtime.

It seems like Inductive (the speaker's start-up) uses unsupervised learning to clean the data. This is the "automated data cleaning". Then humans look at the outputted data later.

The speaker mentions profiler's that collect signals during the automated cleaning process. Then these signals could be fed directly into the prediction model as opposed to being boxed off in a separate data cleaning box.
